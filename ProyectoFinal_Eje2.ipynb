{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 1 Cargar el Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mile1\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\mile1\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "c:\\Users\\mile1\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el dataset\n",
    "data = pd.read_csv('train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 2 Dividir el Data Set en Training y Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el dataset en características (X) y etiquetas (y)\n",
    "X = data.iloc[:, 1:]\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "# Dividir el dataset en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 3. Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Normalizar los valores de píxeles\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir y_train a un arreglo de numpy\n",
    "y_train_array = np.array(y_train)\n",
    "y_test_array= np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar las etiquetas en formato one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train_array.reshape(-1, 1)).toarray()\n",
    "y_test_encoded = encoder.transform(y_test_array.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 4. Creación de la red neuronal artificial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Definir la estructura de la red neuronal\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar la red neuronal\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 5. Entrenamiento de la red neuronal artificial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Definir la estructura de la red neuronal\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar la red neuronal\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 6. Evaluación de la red neuronal artificial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Obtener las predicciones de la red neuronal para el conjunto de prueba\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las predicciones a etiquetas numéricas\n",
    "y_test_pred_labels = np.argmax(y_test_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mile1\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calcular las métricas de clasificación\n",
    "accuracy = accuracy_score(y_test, y_test_pred_labels)\n",
    "precision = precision_score(y_test, y_test_pred_labels, average='macro')\n",
    "recall = recall_score(y_test, y_test_pred_labels, average='macro')\n",
    "f1 = f1_score(y_test, y_test_pred_labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.06904761904761905\n",
      "Precision: 0.03916071347690282\n",
      "Recall: 0.0747279332291608\n",
      "F1 Score: 0.04563305889714333\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las métricas de clasificación\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 7. Red neuronal Convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Definir la estructura de la red neuronal convolucional\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar la red neuronal convolucional\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 8. Entrenamiento de la red convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert DataFrame to NumPy array\n",
    "X_train_array = X_train.values\n",
    "X_test_array = X_test.values\n",
    "\n",
    "# Reshape the input data for CNN\n",
    "X_train_reshaped = X_train_array.reshape(-1, 28, 28, 1)\n",
    "X_test_reshaped = X_test_array.reshape(-1, 28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "525/525 [==============================] - 17s 30ms/step - loss: 1.1631 - accuracy: 0.9122 - val_loss: 0.1565 - val_accuracy: 0.9586\n",
      "Epoch 2/10\n",
      "525/525 [==============================] - 15s 29ms/step - loss: 0.0999 - accuracy: 0.9713 - val_loss: 0.1193 - val_accuracy: 0.9680\n",
      "Epoch 3/10\n",
      "525/525 [==============================] - 14s 28ms/step - loss: 0.0505 - accuracy: 0.9844 - val_loss: 0.1213 - val_accuracy: 0.9713\n",
      "Epoch 4/10\n",
      "525/525 [==============================] - 15s 28ms/step - loss: 0.0383 - accuracy: 0.9882 - val_loss: 0.1434 - val_accuracy: 0.9679\n",
      "Epoch 5/10\n",
      "525/525 [==============================] - 16s 31ms/step - loss: 0.0287 - accuracy: 0.9907 - val_loss: 0.1255 - val_accuracy: 0.9699\n",
      "Epoch 6/10\n",
      "525/525 [==============================] - 15s 28ms/step - loss: 0.0189 - accuracy: 0.9940 - val_loss: 0.1394 - val_accuracy: 0.9700\n",
      "Epoch 7/10\n",
      "525/525 [==============================] - 14s 27ms/step - loss: 0.0405 - accuracy: 0.9885 - val_loss: 0.1309 - val_accuracy: 0.9740\n",
      "Epoch 8/10\n",
      "525/525 [==============================] - 15s 28ms/step - loss: 0.0232 - accuracy: 0.9926 - val_loss: 0.1264 - val_accuracy: 0.9775\n",
      "Epoch 9/10\n",
      "525/525 [==============================] - 15s 29ms/step - loss: 0.0173 - accuracy: 0.9953 - val_loss: 0.1626 - val_accuracy: 0.9725\n",
      "Epoch 10/10\n",
      "525/525 [==============================] - 15s 29ms/step - loss: 0.0221 - accuracy: 0.9924 - val_loss: 0.1398 - val_accuracy: 0.9743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21799bc7bb0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Convert DataFrame to NumPy array\n",
    "X_train_array = X_train.values\n",
    "X_test_array = X_test.values\n",
    "\n",
    "# Reshape the input data for CNN\n",
    "X_train_reshaped = X_train_array.reshape(-1, 28, 28, 1)\n",
    "X_test_reshaped = X_test_array.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Clear the TensorFlow graph\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Define the structure of the convolutional neural network\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the convolutional neural network\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_reshaped, y_train_encoded, batch_size=64, epochs=10, validation_data=(X_test_reshaped, y_test_encoded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 9. Evaluación del modelo convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 1s 4ms/step\n",
      "Accuracy: 0.9742857142857143\n",
      "Precision: 0.9748073000369999\n",
      "Recall: 0.9738955656584759\n",
      "F1 Score: 0.9742588670484628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Obtener las predicciones de la red neuronal convolucional para el conjunto de prueba\n",
    "y_test_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "# Convertir las predicciones a etiquetas numéricas\n",
    "y_test_pred_labels = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "# Calcular las métricas de clasificación\n",
    "accuracy = accuracy_score(y_test, y_test_pred_labels)\n",
    "precision = precision_score(y_test, y_test_pred_labels, average='macro')\n",
    "recall = recall_score(y_test, y_test_pred_labels, average='macro')\n",
    "f1 = f1_score(y_test, y_test_pred_labels, average='macro')\n",
    "\n",
    "# Mostrar las métricas de clasificación\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 10. Comparación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se concluye que el modelo más efectivo es la red neuronal Convolucional, debido al alto de accuracy que nos brinda (0.9742), mientas que la red neuronal artificial es completamente ineficiente con un accuracy de 0.069 para el manejo y clasificación de pixeles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
